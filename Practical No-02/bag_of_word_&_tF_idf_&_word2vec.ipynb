{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag_of_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 29 tokens\n",
      "\n",
      "{'Gensim': 0, 'Python': 1, 'a': 2, 'algorithms.': 3, 'and': 4, 'as': 5, 'designed': 6, 'digital': 7, 'documents': 8, 'efficiently': 9, 'for': 10, 'free': 11, 'is': 12, 'learning': 13, 'library': 14, 'machine': 15, 'open-source': 16, 'painlessly': 17, 'possible.': 18, 'process': 19, 'raw,': 20, 'representing': 21, 'semantic': 22, 'texts': 23, 'to': 24, 'unstructured': 25, 'unsupervised': 26, 'using': 27, 'vectors,': 28}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "text1 = [\"\"\"Gensim is a free open-source Python library for representing documents as semantic vectors,\n",
    "           as efficiently and painlessly as possible. Gensim is designed \n",
    "           to process raw, unstructured digital texts using unsupervised machine learning algorithms.\"\"\"]\n",
    "\n",
    "tokens1 = [[item for item in line.split()] for line in text1]\n",
    "g_dict1 = corpora.Dictionary(tokens1)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
    "print(g_dict1.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 57 tokens\n",
      "\n",
      "{'analyzing': 0, 'and': 1, 'branch': 2, 'consists': 3, 'data': 4, 'deriving': 5, 'efficient': 6, 'for': 7, 'from': 8, 'in': 9, 'information': 10, 'is': 11, 'manner': 12, 'nlp': 13, 'of': 14, 'processes': 15, 'science': 16, 'smart': 17, 'systematic': 18, 'text': 19, 'that': 20, 'the': 21, 'understanding': 22, 'analysis': 23, 'as': 24, 'automated': 25, 'automatic': 26, 'by': 27, 'can': 28, 'chunks': 29, 'components': 30, 'entity': 31, 'etc': 32, 'extraction': 33, 'its': 34, 'machine': 35, 'massive': 36, 'named': 37, 'numerous': 38, 'one': 39, 'organize': 40, 'perform': 41, 'problems': 42, 'range': 43, 'recognition': 44, 'relationship': 45, 'segmentation': 46, 'sentiment': 47, 'solve': 48, 'speech': 49, 'such': 50, 'summarization': 51, 'tasks': 52, 'topic': 53, 'translation': 54, 'utilizing': 55, 'wide': 56}\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "text2 = open('sample_text.txt', encoding ='utf-8')\n",
    " \n",
    "tokens2 =[]\n",
    "for line in text2.read().split('.'):\n",
    "  tokens2.append(simple_preprocess(line, deacc = True))\n",
    "\n",
    "g_dict2 = corpora.Dictionary(tokens2)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(g_dict2)) + \" tokens\\n\")\n",
    "print(g_dict2.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 81 tokens\n",
      "\n",
      "{'Gensim': 0, 'Python': 1, 'a': 2, 'algorithms.': 3, 'and': 4, 'as': 5, 'designed': 6, 'digital': 7, 'documents': 8, 'efficiently': 9, 'for': 10, 'free': 11, 'is': 12, 'learning': 13, 'library': 14, 'machine': 15, 'open-source': 16, 'painlessly': 17, 'possible.': 18, 'process': 19, 'raw,': 20, 'representing': 21, 'semantic': 22, 'texts': 23, 'to': 24, 'unstructured': 25, 'unsupervised': 26, 'using': 27, 'vectors,': 28, 'analyzing': 29, 'branch': 30, 'consists': 31, 'data': 32, 'deriving': 33, 'efficient': 34, 'from': 35, 'in': 36, 'information': 37, 'manner': 38, 'nlp': 39, 'of': 40, 'processes': 41, 'science': 42, 'smart': 43, 'systematic': 44, 'text': 45, 'that': 46, 'the': 47, 'understanding': 48, 'analysis': 49, 'automated': 50, 'automatic': 51, 'by': 52, 'can': 53, 'chunks': 54, 'components': 55, 'entity': 56, 'etc': 57, 'extraction': 58, 'its': 59, 'massive': 60, 'named': 61, 'numerous': 62, 'one': 63, 'organize': 64, 'perform': 65, 'problems': 66, 'range': 67, 'recognition': 68, 'relationship': 69, 'segmentation': 70, 'sentiment': 71, 'solve': 72, 'speech': 73, 'such': 74, 'summarization': 75, 'tasks': 76, 'topic': 77, 'translation': 78, 'utilizing': 79, 'wide': 80}\n"
     ]
    }
   ],
   "source": [
    "g_dict1.add_documents(tokens2)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
    "print(g_dict1.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)]]\n"
     ]
    }
   ],
   "source": [
    "g_bow =[g_dict1.doc2bow(token, allow_update = True) for token in tokens1]\n",
    "print(\"Bag of Words : \", g_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Dictionary and BOW\n",
    "g_dict1.save('/content/drive/MyDrive/gensim_tutorial/g_dict1.dict') \n",
    "corpora.MmCorpus.serialize('/content/drive/MyDrive/gensim_tutorial/g_bow1.mm', g_bow)  \n",
    "\n",
    "# Load the Dictionary and BOW\n",
    "g_dict_load = corpora.Dictionary.load('/content/drive/MyDrive/gensim_tutorial/g_dict1.dict')\n",
    "g_bow_load = corpora.MmCorpus('/content/drive/MyDrive/gensim_tutorial/g_bow1.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary : \n",
      "[['be', 1], ['better', 1], ['but', 1], ['can', 1], ['excellent', 1], ['food', 1], ['is', 1], ['service', 1], ['the', 2]]\n",
      "[['food', 1], ['is', 1], ['service', 1], ['the', 2], ['always', 1], ['and', 1], ['delicious', 1], ['loved', 1]]\n",
      "[['food', 1], ['service', 1], ['the', 2], ['and', 1], ['mediocre', 1], ['terrible', 1], ['was', 2]]\n",
      "TF-IDF Vector:\n",
      "[['be', 0.43], ['better', 0.43], ['but', 0.43], ['can', 0.43], ['excellent', 0.43], ['food', 0.09], ['is', 0.21], ['service', 0.09], ['the', 0.18]]\n",
      "[['food', 0.11], ['is', 0.26], ['service', 0.11], ['the', 0.21], ['always', 0.52], ['and', 0.26], ['delicious', 0.52], ['loved', 0.52]]\n",
      "[['food', 0.08], ['service', 0.08], ['the', 0.16], ['and', 0.2], ['mediocre', 0.39], ['terrible', 0.39], ['was', 0.78]]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models  # Import the required Gensim modules\n",
    "from gensim.utils import simple_preprocess  # Import simple_preprocess to tokenize text\n",
    "import numpy as np  # Import numpy for rounding\n",
    "\n",
    "# Input text\n",
    "text = [\"The food is excellent but the service can be better\",\n",
    "        \"The food is always delicious and loved the service\",\n",
    "        \"The food was mediocre and the service was terrible\"]\n",
    "\n",
    "# Create dictionary and Bag of Words (BoW)\n",
    "g_dict = corpora.Dictionary([simple_preprocess(line) for line in text])\n",
    "g_bow = [g_dict.doc2bow(simple_preprocess(line)) for line in text]\n",
    "\n",
    "# Print dictionary\n",
    "print(\"Dictionary : \")\n",
    "for item in g_bow:\n",
    "    print([[g_dict[id], freq] for id, freq in item])\n",
    "\n",
    "# Create TF-IDF model\n",
    "g_tfidf = models.TfidfModel(g_bow, smartirs='ntc')\n",
    "\n",
    "# Print TF-IDF Vector\n",
    "print(\"TF-IDF Vector:\")\n",
    "for item in g_tfidf[g_bow]:\n",
    "    print([[g_dict[id], np.around(freq, decimals=2)] for id, freq in item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "words = [d for d in dataset]\n",
    "\n",
    "data1 = words[:1000]\n",
    "w2v_model = Word2Vec(data1, min_count = 0, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.save('/content/drive/MyDrive/gensim_practice/w2v_model1')\n",
    "w2v_model = Word2Vec.load('/content/drive/MyDrive/gensim_practice/w2v_model1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2379209e+00 -5.4677814e-01  7.0871346e-02 -8.0347133e-01\n",
      " -5.1042616e-01 -1.9007365e-01 -8.4404242e-01  8.3497989e-01\n",
      "  3.1455657e+00  4.3382388e-01  3.0204952e-01 -1.9370946e-01\n",
      " -2.8551652e+00 -1.0989660e+00 -1.5059642e-01 -1.2903551e+00\n",
      "  5.1325428e-01  1.3914520e+00  8.8145250e-01  1.3453034e+00\n",
      "  6.4017677e-01 -3.5136471e+00  5.0670558e-01 -1.3250927e+00\n",
      "  1.6346176e+00 -2.9696965e+00 -5.7190365e-01 -8.1369895e-01\n",
      "  5.8513250e-02  4.9608046e-01  1.3150169e+00 -9.4212592e-01\n",
      " -4.4825709e-01 -5.1767349e-01  2.2834013e+00 -4.5724735e+00\n",
      " -7.3194891e-01  5.7519025e-01  1.2337620e+00  2.6014505e-02\n",
      " -9.0268064e-01 -3.5901758e-01 -1.4015324e+00 -7.0532137e-01\n",
      " -2.5445610e-01  3.2976193e+00  1.0291837e+00 -1.1839085e+00\n",
      "  1.3408244e+00  1.5815179e+00  6.1444885e-01  1.5739673e+00\n",
      "  4.2253137e+00  3.1174617e+00  1.3782628e+00  1.9068192e+00\n",
      " -8.5272139e-01 -2.8659216e-01 -1.9584981e+00  2.7572617e-01\n",
      "  6.0621783e-02  1.6343908e+00  1.5055938e+00 -4.2271824e+00\n",
      " -1.1717790e+00 -6.9866705e-01  2.3563719e+00  5.3249317e-01\n",
      " -1.6318636e+00  2.9396377e+00 -2.1094415e+00 -2.1175411e+00\n",
      "  5.4558575e-01  2.6877699e+00  8.8974839e-01  1.9468713e-01\n",
      " -1.0316647e+00  3.4373947e-03 -4.4741359e-01 -1.5917227e+00\n",
      "  2.8241912e-01 -1.7468512e+00  3.5732090e+00 -1.6464545e-01\n",
      "  6.8899769e-01  1.2929269e+00  3.5860169e-01 -2.1318798e+00\n",
      " -9.6357584e-01 -1.2064433e-01  2.2609985e+00 -9.5162278e-01\n",
      "  1.4174705e+00  4.1787559e-01  1.1832699e+00 -7.2359842e-01\n",
      "  7.9596895e-01  1.3054043e+00 -1.2893946e+00  1.1937895e+00]\n"
     ]
    }
   ],
   "source": [
    "data2 = words[1000:]  # Taking a slice of the data\n",
    "w2v_model.build_vocab(data2, update=True)  # Updating the vocabulary with new data\n",
    "w2v_model.train(data2, total_examples=w2v_model.corpus_count, epochs=10)  # Set the desired number of epochs\n",
    "print(w2v_model.wv['social'])  # Accessing the word vector for 'social'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
